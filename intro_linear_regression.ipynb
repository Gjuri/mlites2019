{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro_linear_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlites/mlites2019/blob/master/intro_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7YzKFlBhrNw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Linear Regression and Gradient Descent\n",
        "\n",
        "Hello! In this exercise we'll peek behind the scenes at the deceptively simple linear regression algorithm by implementing a version using Gradient Descent.\n",
        "\n",
        "Along the way we'll learn to do some data manipulation in python using Pandas and NumPy, make some plots using MatPlotLib, and learn how to separate our data into Training and Test sets for validation.\n",
        "\n",
        "If you haven't already, make sure you run [intro_kaggle.ipynb](https://colab.research.google.com/github/mlites/mlites2019/blob/master/kaggle_introduction.ipynb) to download the necessary datasets\n",
        "\n",
        "\n",
        "Some more background\n",
        "* [Introduction to Machine Learning Algorithms: Linear Regression](https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a)\n",
        "\n",
        "* [Quick start to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)\n"
      ]
    },
    {
      "metadata": {
        "id": "HiNCUqcAs-Ci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "\n",
        "1. import necessary packages\n",
        "2. Load Iditarod data\n",
        "3. Do some date and time conversion\n",
        "4. Generate some new features\n",
        "5. Plot some data\n",
        "6. Fit regression using ordinary least squares\n",
        "7. Fit regression with sklearn\n",
        "8. Fit regression with gradient descent"
      ]
    },
    {
      "metadata": {
        "id": "1SVLr4yuG45H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting Started"
      ]
    },
    {
      "metadata": {
        "id": "bpUxrh72gEvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd #import Pandas under the shortcut 'pd'\n",
        "import numpy as np #import NumPy; NumPy is the fundamental package for scientific computing with Python\n",
        "import matplotlib.pyplot as plt #MatPlotLib is a plotting library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CeYCbkx0G7WH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exploring the Iditarod Dataset"
      ]
    },
    {
      "metadata": {
        "id": "nyu1CYGKtypu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# first lets load the Iditarod data into a pandas dataframe in a variable called 'iditarod'\n",
        "\n",
        "iditarod = pd.read_csv(\"iditarod.csv\")\n",
        "iditarod.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HP1WAYeruSD0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is a bunch of data here about each musher, like when they arrived at each checkpoint, how long it took, what their speed was, and how many dogs they checked in or out.\n",
        "\n",
        "Let's explore the data a bit to see what it looks like. Say we're interested in how the number of dogs on the team affects the speed, we can plot that using the MatPlotLib library.\n",
        "\n",
        "Let's look at the speed versus the number of dogs dropped by each team."
      ]
    },
    {
      "metadata": {
        "id": "p8VbufoWuunJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iditarod[\"Dogs_dropped\"] = 16 - iditarod[\"Arrival Dogs\"] #everyone starts with 16 dogs\n",
        "plt.figure(figsize=(10,10)) #set the figure size in inches\n",
        "plt.scatter(iditarod[\"Dogs_dropped\"],iditarod[\"Speed\"],color='blue') # the scatter command make a scatter plot\n",
        "plt.xlabel(\"Dogs Dropped\")\n",
        "plt.ylabel(\"Speed (mph)\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qUq0xjo8v3Uk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "* What do you think, is there a trend?\n",
        "* Do mushers go faster when they have more dogs?\n",
        "* What other factors might affect their speed?\n",
        "\n",
        "\n",
        "Let's do some feature conversions and dig into the data a bit more."
      ]
    },
    {
      "metadata": {
        "id": "wcBdzJf9xBXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#let's drop values that have NA or NaN's\n",
        "iditarod = iditarod.dropna()\n",
        "\n",
        "#then convert the times into machine-readable timestamps\n",
        "\n",
        "# import the datetime module\n",
        "from datetime import datetime\n",
        "\n",
        "# get the Date and Time into a new temporary dataframe\n",
        "# pd.loc[] accesses rows or columns in a dataframe by character labels\n",
        "dt = iditarod.loc[:, ['Arrival Date', 'Arrival Time']] # ':' means 'all rows'\n",
        "\n",
        "# 'apply' applies a function (in this case an anonymous 'lambda' function) to each row (axis=1) of the new dataframe\n",
        "# the function concatenates the Date and Time and converts them into a special 'datetime' object\n",
        "# which then gets saved back into the original dataframe\n",
        "\n",
        "iditarod['Arrival_datetime'] = dt.apply(lambda x: datetime.strptime(x[0] + '/' + x[1], '%m/%d/%Y/%H:%M:%S'), axis=1)\n",
        "\n",
        "# another way to make datetime objects in pandas\n",
        "# iditarod[\"Arrival Time\"] = pd.DatetimeIndex(iditarod[\"Arrival Time\"],dtype='datetime64[ns]')\n",
        "# iditarod[\"Departure Time\"] = pd.DatetimeIndex(iditarod[\"Departure Time\"],dtype='datetime64[ns]')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2xChmo_qYmdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can plot the number of Dogs Dropped as a function of the Datetime"
      ]
    },
    {
      "metadata": {
        "id": "iGqwFqOGYnET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot_date is a matplotlib command to use when you're plotting a date or time\n",
        "plt.figure(figsize=(10,10)) #set the figure size in inches\n",
        "plt.plot_date(iditarod[\"Arrival_datetime\"],iditarod[\"Dogs_dropped\"],color='blue') #make a scatter plot\n",
        "plt.xlabel(\"Datetime\")\n",
        "plt.ylabel(\"Dogs Dropped\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "soMC-duB8yGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Is there a trend here?\n",
        "\n",
        "It kind of looks like it, but how can we tell for sure?\n",
        "\n",
        "Let's look at one more example."
      ]
    },
    {
      "metadata": {
        "id": "gvA-Waw16bLh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# let's find the cumulative time and distance traveleled to each checkpoint\n",
        "\n",
        "# first we'll find the unique checkpoints like this:\n",
        "iditarod[\"Checkpoint\"].unique()\n",
        "\n",
        "# then from the Iditarod website we can find the cumulative distances\n",
        "# to each unique checkpoint\n",
        "# https://iditarod.com/race-map/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wa-S6vlU922",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# PS the race started in Fairbanks in 2017 because of global warming\n",
        "\n",
        "# inputing them into a new dictionary ('dict') like so:\n",
        "cumulative_dist = {\n",
        "'Fairbanks'\t: 0,\n",
        "'Nenana'\t: 71,\n",
        "'Manley'\t: 161,\n",
        "'Tanana'\t: 227,\n",
        "'Ruby'\t: 346,\n",
        "'Galena'\t: 396,\n",
        "'Huslia'\t: 478,\n",
        "'Koyukuk'\t: 564,\n",
        "'Nulato'\t: 586,\n",
        "'Kaltag'\t: 633,\n",
        "'Unalakleet'\t: 718,\n",
        "'Shaktoolik'\t: 758,\n",
        "'Koyuk'\t: 808,\n",
        "'Elim'\t: 856,\n",
        "'White Mountain'\t: 902,\n",
        "'Safety'\t: 957,\n",
        "'Nome'\t: 979\n",
        "}\n",
        "\n",
        "# now we can insert those into the iditarod dataframe using the 'replace()' function like so:\n",
        "# this function replacess e.g. \"Huslia\" with \"478\" from the original column (\"Checkpoint\")\n",
        "iditarod[\"Distance_traveled\"] = iditarod[\"Checkpoint\"].replace(cumulative_dist)\n",
        "\n",
        "# finally lets convert the timestamps into hours elapsed since the the start of the race\n",
        "# we subtract the Arrival time from the race start time and use the .astype() function to report the time difference in hours\n",
        "# I know what you're saying -- this isn't perfectly accurate because they actually do a 2-minute staggered start, but I'll leave that as an exercise for the reader\n",
        "\n",
        "racestart = pd.Timestamp('2017-03-06 11:00:00.00000')\n",
        "iditarod[\"Elapsed_time\"] = (iditarod[\"Arrival_datetime\"] - racestart).astype('timedelta64[h]')\n",
        "\n",
        "# now let's take a look at our new features\n",
        "iditarod.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a-q6XmD9DOjA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, your turn!\n",
        "\n",
        "Fill in the missing details to make a scatter plot of the Elapsed Time as a function of the Distance Traveled"
      ]
    },
    {
      "metadata": {
        "id": "BdMJcJFwATW4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10)) #set the figure size in inches\n",
        "plt.scatter(iditarod[\"\"],iditarod[\"\"],color='blue') #make a scatter plot\n",
        "plt.xlabel(\"Distance Traveled\")\n",
        "plt.ylabel(\"Elapsed Time\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jjIupi5SX5-r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You'll notice that this dataset violates several rules of traditional statistical analysis:\n",
        "* The sample mean and variances are not independent, in fact they are correlated\n",
        "* The samples are not i.i.d. or \"independent and identically distributed\", since each time point 't' depends on 't-1'\n",
        "* Which means this is a time series, though for now we're going to ignore this\n",
        "* Here's a nice online book about analysing time series data properly:\n",
        "    * [Forecasting: principles and practice](https://otexts.com/fpp2/stationarity.html)\n",
        "* [How (not) to use Machine Learning on Time Series Forecasting](https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424)"
      ]
    },
    {
      "metadata": {
        "id": "aApvHl2MHDNe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introducing Training and Testing Sets\n",
        "\n",
        "In Machine Learning, the way we validate how well the machines are learning is by using two subsets of any given dataset: a Train set, and a Test set. Let's say we split the data 80/20 between these subsets, this means we'll usen80% of the data to Train our model, and reserve 20% to Test whether our predictions are any good.\n",
        "\n",
        "\n",
        "**Some questions to ponder:**\n",
        "\n",
        "* Why don't we train on the whole data set?\n",
        "* Wouldn't we get a better model that way?\n"
      ]
    },
    {
      "metadata": {
        "id": "1L2Z8NRxIA_Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's work with the Elapsed Time and Distance Traveled data, and split it into Train and Test sets.\n",
        "\n",
        "We'll use scikit-learn, a simple but powerful machine learning toolkit for python.\n",
        "\n",
        "docs: [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ]
    },
    {
      "metadata": {
        "id": "MXCGftTFt5tt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn #import the scikit-learn package\n",
        "from sklearn.model_selection import train_test_split #import the train_test_split function\n",
        "\n",
        "x = iditarod[\"Distance_traveled\"]\n",
        "y = iditarod[\"Elapsed_time\"]\n",
        "\n",
        "# here we tell sklearn to split our data and reserve 20% for the test set\n",
        "# train_test_split automatically shuffles the data before returning the splits\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "# we need to convert the data to numpy array and reshape it for input into the linear model function\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "x_train = x_train.reshape(-1,1) # the -1 indicates that that dimension will be inferred from the data\n",
        "x_test = x_test.reshape(-1,1) # so (-1,1) says to reshape the data into 1 column that has the same number of rows as the data\n",
        "y_train = y_train.reshape(-1,1) # the -1 indicates that that dimension will be inferred from the data\n",
        "y_test = y_test.reshape(-1,1) # so (-1,1) says to reshape the data into 1 column that has the same number of rows as the data\n",
        "\n",
        "# mimic \"head\" to preview the x_test set by taking a top slice of the array\n",
        "print(x_test[:10]) # notice x_test values are integers\n",
        "\n",
        "# to \"tail\" take a bottom slice\n",
        "print(y_test[-10:]) # notice y_test values are floats\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64IcRWxPtJ3j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "It turns out, for linear regression it is often possible to calculate the best possible coefficients exactly.\n",
        "\n",
        "### Ordinary Least Squares\n",
        "Here's an example implementing an exact solution to the linear regression problem using Ordinary Least Squares.\n",
        "\n",
        "However, since this is not possible for every function, we will also use machine learning, and will go through an example using gradient descent from scratch.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KrFemk57A6sF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#generate some random correlated data\n",
        "n = 20\n",
        "x = np.random.rand(n)\n",
        "y = np.random.normal(loc=x,scale=0.2,size=n)\n",
        "dy = x - y\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.scatter(x,y,label=\"observed\",color=\"black\")\n",
        "ax.plot(x,x,label=\"predicted\",color=\"orange\")\n",
        "ax.vlines(x,y,y+dy,label=\"residuals\",color=\"blue\")\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q4ocT0ODFSMr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the plot above, the orange line is our *estimator* or *best fit line*, which we want to optimize to best predict future values of $y$ given $x$.\n",
        "\n",
        "The blue lines are *residuals*, i.e. the difference between the predicted and actual values for any given $x$.\n",
        "\n",
        "We'll use the following shorthand:\n",
        "* $x_i$ is the $i$th value at which a measurement of $y$ was taken\n",
        "* $y_i$ is the observed value at $x_i$\n",
        "* $r_i$ is the residual error at $x_i$\n",
        "* $\\hat y_i$ is the predicted value at $x_i$\n",
        "* $\\bar x = \\frac{1}{n}\\sum_{i=1}^{n} x_i$, the mean of the $x$s\n",
        "* $\\bar y= \\frac{1}{n}\\sum_{i=1}^{n} y_i$, the mean of the $y$s\n",
        "\n",
        "\n",
        "We can calculate the value $y_i$ at any given point $x_i$ given the following equation\n",
        "\n",
        "$$y_i = mx_i + b + r_i$$\n",
        "\n",
        "The goal of simple linear regression is to find the line \n",
        "$$\\hat y_i = mx_i+b$$ that optimizes our ability to predict future $y$s given future $x$s.\n",
        "\n",
        "It turns out that for simple linear regression (that is, when there is only one input and one output variable), the best estimates of the slope ($m$) and intercept ($b$) can be obtained by minimizing the sum of the squared errors of prediction.\n",
        "\n",
        "$$SSE = \\sum_{i=1}^{n}  r_i^2 = \\sum_{i=1}^{n} (y_i - \\hat y_i)^2$$\n",
        "\n",
        "Ideally, we want to find the values of $m$ and $b$ that minimize the value of the SSE function. Mathematically, you can do that by expressing SSE in terms of $m$ and $b$, taking the derivatives of SSE with respect to $m$ and $b$, setting these derivatives to zero, and solving for $m$ and $b$. However, we won't go into that derivation here and jump straight to the conclusions, which are:\n",
        "\n",
        "$$m = \\frac{Cov(x,y)}{Var(x)}$$\n",
        "\n",
        "where $Cov(x,y)$ is the covariance:\n",
        "\n",
        "$$Cov(x,y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar x)(y_i - \\bar y)$$\n",
        "\n",
        "and $Var(x)$ is the variance:\n",
        "\n",
        "$$Var(x) = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar x)^2$$\n",
        "\n",
        "and\n",
        "\n",
        "$$b = \\bar y - m \\cdot \\bar x$$\n",
        "\n",
        "\n",
        "In the example below, we'll calculate these values for the training set"
      ]
    },
    {
      "metadata": {
        "id": "6pwcS8EnhVoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# means of x and y\n",
        "xbar = np.mean(x_train)\n",
        "ybar = np.mean(y_train)\n",
        "\n",
        "# calculate Cov(x,y) and Var(x)\n",
        "cov_xy = (1/n)*np.sum((x_train - mean_x) * (y_train - mean_y))\n",
        "var_x = (1/n)*np.sum((x_train - mean_x) ** 2)\n",
        "\n",
        "# get the estimates of the coefficients using Ordinary Least Squares\n",
        "m = cov_xy / var_x\n",
        "b = ybar - (m * xbar)\n",
        "\n",
        "# Print the coefficients\n",
        "print(\"y = %.2fx + %.2f\" % (m, b))\n",
        "\n",
        "# import the r2_score module from sklearn to calculate the R^2 metric\n",
        "from sklearn.metrics import r2_score # import the r2_score module from sklearn\n",
        "\n",
        "# compare our test set to our training set\n",
        "y_pred = m*x_test + b\n",
        "print('R2 Score:',r2_score(y_test,y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "94sYagfURxsv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These are the gold standard coefficients and R2 value that we'll compare the other methods to below."
      ]
    },
    {
      "metadata": {
        "id": "drCVpaAAZ7CX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## sklearn LinearRegression\n",
        "\n",
        "Now that we've split the data into training and testing sets, let's do some linear regression. We'll use the LinearRegression module from sklearn first."
      ]
    },
    {
      "metadata": {
        "id": "RZJGwk_Hte_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression # import the LinearRegression module from sklearn\n",
        "\n",
        "clf = LinearRegression() # initiate an object of Class LinearRegression\n",
        "clf.fit(x_train,y_train) # call the fit() function to fit the training data x_train and y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vo4Z_uauUlnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's it!\n",
        "\n",
        "Let's print out the formula for our best fit line using the coefficients found within the 'clf' object"
      ]
    },
    {
      "metadata": {
        "id": "cSHUv9ufOU5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the formula for the best fit line is\n",
        "slope = clf.coef_[0]\n",
        "intercept = clf.intercept_\n",
        "print(\"y = \" + str(slope[0]) + \"x\" + \" + \" + str(intercept[0])) #str() converts a numeric to a string\n",
        "\n",
        "# get control of those significant figures by printing with nicer formatting\n",
        "print(\"y = %.2fx + %.2f\" % (slope, intercept)) # %.2f says to format a _f_loating-point number to 2 decimal places"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5YMSdkc2UbDU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have a model trained on the training set, we can use it to predict new values.\n",
        "\n",
        "For example, if the Iditarod was 1100 or 1200 miles long instead of 1000, how many hours would it take the average musher to finish?\n"
      ]
    },
    {
      "metadata": {
        "id": "p502J_nCUdQt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict([[1100],[1200]]) # we need double brackets because predict() expects an array of arrays\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LASjK74aQg7C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's plot the best fit line over the training and testing data"
      ]
    },
    {
      "metadata": {
        "id": "t7V6HKM5QgjP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_fit = np.linspace(0,1000, 50) # linspace(start, stop, num) generates a linearly-spaced numeric array\n",
        "y_fit = slope*x_fit + intercept\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.subplot(1, 2, 1) #subplot(nrows, ncols, index) allows for multiple plots on one page\n",
        "plt.scatter(x_train,y_train,color='red',label='train')\n",
        "plt.plot(x_fit,y_fit,color='black',label = 'prediction')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Distance Traveled\")\n",
        "plt.ylabel(\"Elapsed Time\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(x_test,y_test,color='blue',label='test')\n",
        "plt.plot(x_fit,y_fit,color='black',label = 'prediction')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Distance Traveled\")\n",
        "plt.ylabel(\"Elapsed Time\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SbtAJkcOaf-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's use the Test set to validate how well we did on our predictions.\n",
        "\n",
        "We'll use the R<sup>2</sup> test to do so along with the sklearn predict() function\n"
      ]
    },
    {
      "metadata": {
        "id": "-5CTzFGitnrz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# first we'll use the sklearn predict() function\n",
        "y_pred = clf.predict(x_test)\n",
        "print('R2 Score:',r2_score(y_test,y_pred))\n",
        "\n",
        "# if we wanted to do the prediction manually we could do it like this\n",
        "y_pred = slope * x_test + intercept\n",
        "print('R2 Score:',r2_score(y_test,y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KXC39Izra2Ia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Not too bad!\n",
        "\n",
        "**Question:** What features might we add to make our predictions better next time?"
      ]
    },
    {
      "metadata": {
        "id": "McnfTUP5a9e4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Regression by Gradient Descent\n",
        "\n",
        "\"Training a machine learning algorithm or a neural network is really just the process of minimizing the cost function\"\n",
        "\n",
        "Let's see how that works under the hood\n",
        "\n",
        "\n",
        "Some background info:\n",
        "  * [Linear Regression in Python](https://towardsdatascience.com/linear-regression-in-python-9a1f5f000606)\n",
        "  \n",
        "  * [Linear Regression Using Gradient Descent in 10 Lines of Code](https://towardsdatascience.com/linear-regression-using-gradient-descent-in-10-lines-of-code-642f995339c0)\n",
        "  \n",
        "  *  [Linear Regression using Gradient Descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)\n",
        "\n",
        "* This [video](https://www.youtube.com/embed/yFPLyDwVifc) from Andrew Ng's Machine Learning course at Stanford may be useful\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kfIE29IeMpmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Derivation\n",
        "\n",
        "The nature of the gradient descent method is to iteratively update our best guesses for each parameter by calculating the gradients (or partial derivatives) of the system at each iteration for each parameter. Then we use the gradients to adjust the direction and rate of travel through the parameter space.\n",
        "\n",
        "\n",
        "In the following derivation we'll define $y_i$ as the observed value at $x_i$ and $\\hat{y}_i$ as the predicted value at $x_i$\n",
        "\n",
        "\n",
        "Our linear equation for prediction is: $$\\hat{y}_i = mx_i + b$$\n",
        "\n",
        "\n",
        "For our Error (or Loss, or Cost) function we'll use the Root Mean Square Error (RMSE), which is the standard deviation of the residuals (prediction errors). This isn't the only possible Error function and depending on the application, several others are used in machine learning.\n",
        "\n",
        "$$E =\\frac {1}{n}\\sum(y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "To calculate the gradients, we'll need to take the partial derivatives with respect to each of the parameters we'll be adjusting, in this case $m$ (the slope) and $b$ (the intercept). Let's walk through the derivation.\n",
        "\n",
        "Substituting for $\\hat{y}_i$, we get\n",
        "$$E =\\frac {1}{n}\\sum \\big(y_i - (mx_i + b)\\big)^2$$\n",
        "\n",
        "Expanding, we get\n",
        "$$E =\\frac {1}{n}\\sum\\big(y_i - (mx_i + b)\\big)\\big(y_i - (mx_i + b)\\big)$$\n",
        "\n",
        "and  FOILing we get\n",
        "$$E =\\frac {1}{n}\\sum y_i^2 - 2y_i(mx_i + b) + (mx_i+b)^2$$\n",
        "\n",
        "and just for clarity let's multiply the whole thing out\n",
        "$$E =\\frac {1}{n}\\sum y_i^2 - 2mx_iy_i - 2by_i +m^2x_i^2 + 2bmx_i+b^2$$\n",
        "\n",
        "Then to get the gradients with respect to $m$ and $b$ we take the partial derivatives w.r.t. each, remembering that constants differentiate to 0\n",
        "\n",
        "$$\\frac{\\partial E}{\\partial m} = \\frac {1}{n}\\sum 0 - 2x_iy_i - 0 + 2mx_i^2 + 2bx_i + 0$$\n",
        "\n",
        "Pulling out a factor of $-2x_i$, we get\n",
        "\n",
        "$$\\frac{\\partial E}{\\partial m} = \\frac {1}{n}\\sum -2x_i(y_i - mx_i - b)$$\n",
        "\n",
        "and finally rearranging and using a simpler functional notation, we get\n",
        "$$D_m = -\\frac {2}{n}\\sum x_i\\big(y_i-(mx_i +b)\\big)$$\n",
        "\n",
        "which could also be written\n",
        "$$D_m = -\\frac {2}{n}\\sum x_i\\big(y_i-\\hat y_i\\big)$$\n",
        "\n",
        "As for $b$, when we take the partial derivative we get\n",
        "$$\\frac{\\partial E}{\\partial b} =\\frac {1}{n}\\sum 0 - 0 - 2y_i + 0 + 2mx_i+2b$$\n",
        "\n",
        "\n",
        "Pulling out a factor of $-2$ and rearranging as before we get\n",
        "$$D_b = -\\frac{2}{n}\\sum \\big(y_i - (mx_i + b)\\big)$$\n",
        "\n",
        "which could also be written\n",
        "$$D_b = -\\frac {2}{n}\\sum \\big(y_i-\\hat y_i\\big)$$\n",
        "\n",
        "\n",
        "Now we can use these gradients to update our guesses using the following equations, where $L$ is the *learning rate*. Again, this is just one possible way to update our guesses, and other more sophisticated ways are often used in machine learning.\n",
        "\n",
        "$$m = m - L \\cdot Dm $$\n",
        "$$b = b - L \\cdot Db $$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vr3pgoRW6KnB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ]
    },
    {
      "metadata": {
        "id": "WpVIcmNztk4p",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "## Linear Regression by gradient descent using least squares error\n",
        "\n",
        "# let's define a function that implements gradient descent for linear regression\n",
        "# using the error criterion E = (1/n)*sum((y_i - yhat_i)^2)\n",
        "\n",
        "def linear_regression_lse(x_i, y_i, m=0, b=0, L=1e-6, epochs=1000, plotevery=100):\n",
        "  grid = widgets.Grid(1, 1) #setup colab grid output\n",
        "  n = float(len(y_i)) #calculate length of input data\n",
        "  Esave = np.full(epochs, np.nan) #initialize an array of 'epochs' length with NaNs\n",
        "  for i in range(epochs): #perform loop 'epochs' times\n",
        "      yhat_i = m * x_i + b #calculate predictions using current guesses\n",
        "      E = (1/n)*np.sum((y_i - yhat_i)**2) #calculate the cost function for display\n",
        "      Esave[i] = math.log(E) #save the logarithm of the cost function\n",
        "\n",
        "      # now we update our guesses\n",
        "      # note that we always update the values simultaneously rather than sequentially\n",
        "      Dm = (-2/n) * np.sum(x_i * (y_i - (m * x_i + b))) #calculate the gradient of (derivative w.r.t.) the slope\n",
        "      Db = (-2/n) * np.sum(y_i - (m * x_i + b)) #calculate the gradient of (derivative w.r.t.) the intercept\n",
        "      m = m - L * Dm # update the slope\n",
        "      b = b - L * Db # update the intercept\n",
        "      \n",
        "      # do some plotting so we can see how we're doing\n",
        "      if(i%plotevery == 0 or i+1 == epochs or i==0): #plot every 'plotevery' iterations\n",
        "        with grid.output_to(0, 0): #tell colab to plot to a certain grid cell\n",
        "          grid.clear_cell() #clear the grid cell to plot to\n",
        "          plt.figure(figsize=(12,6)) #plot a new figure with width=12 height=6\n",
        "          plt.subplot(1, 2, 1) # slot first figure into index 1 of a 1-row 2-column grid\n",
        "          plt.scatter(x_i,y_i,color=\"red\",label='y_i') #plot the observed points\n",
        "          plt.plot(x_i,yhat_i,color='black',label = 'yhat_i') #plot the prediction line\n",
        "          plt.legend() #plot the legend\n",
        "          plt.xlabel(\"Distance Traveled\") #plot x label\n",
        "          plt.ylabel(\"Elapsed Time\") #plot y label\n",
        "          plt.title(\"step=\" + str(i) + \"   log(cost)=%.5f\" % Esave[i] + \"   y = %.2fx + %.2f\" % (m, b)) # plot title\n",
        "          plt.subplot(1, 2, 2) # slot second figure into index 2 of a 1-row 2-column grid\n",
        "          plt.scatter(range(len(Esave)),Esave) #plot the costs\n",
        "          plt.xlabel(\"iterations\")\n",
        "          plt.ylabel(\"log(E)\")\n",
        "          try: #try this...\n",
        "            plt.ylim(Esave[i],Esave[i-round(i/1.11)]) #scale the y axis to fit the last 90% of the points\n",
        "          except: #but if it doesn't work, do this...\n",
        "            plt.ylim(0,max(Esave)) #scale the y axis to all of the points\n",
        "\n",
        "            #           time.sleep(0.5) #sleep for half a second if it's updating too fast\n",
        "\n",
        "        # when we're done with all the iterations, return the requested values\n",
        "  return m, b, E\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N2w3vJQKiqqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# widgets are some Colab-specific tools\n",
        "# https://colab.research.google.com/notebooks/widgets.ipynb\n",
        "from google.colab import widgets\n",
        "import math # the math library contains useful functions like math.log()\n",
        "\n",
        "lg = linear_regression_lse(x_train, y_train, L=1e-6, epochs=100, plotevery=5)\n",
        "print(\"y = %.2fx + %.2f\" % (lg[0], lg[1]))\n",
        "\n",
        "y_pred = lg[0]*x_test + lg[1]\n",
        "print('R2 Score:',r2_score(y_test,y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmspfEVMdUQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions**:\n",
        "* How does the R2 score compare to the other methods?\n",
        "* What might we do to get a better fit?\n",
        "* If you have an idea, try it!\n"
      ]
    },
    {
      "metadata": {
        "id": "lColI4mdekGJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is some further reading material\n",
        "* [Ordinary Least Square Method](https://mubaris.com/posts/linear-regression/)\n",
        "* [Learning Python Regression Analysis](https://medium.com/@dhwajraj/python-regression-analysis-part-3-ordinary-least-squares-d419322c8da2)\n",
        "* [Linear Regression with NumPy](https://www.cs.toronto.edu/~frossard/post/linear_regression/)\n"
      ]
    },
    {
      "metadata": {
        "id": "VT_68z0Y6WtW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Well done, we learned a lot in this lesson!\n",
        "\n",
        "* You used pandas, numpy, matplotlib, and sklearn, all of which are common packages that we will see over and over again in ML\n",
        "* You read data from a CSV file into a pandas dataframe\n",
        "* You calculated the mean, variance, and covariance of some data\n",
        "* You generated new features from existing features\n",
        "* You converted dates and times\n",
        "* You generated plots using scatter() and plot_date()\n",
        "* You generated a dictionary and used replace() to make substitutions in it\n",
        "* You split your data into train and test sets using sklearn\n",
        "* You ran a linear regression in sklearn\n",
        "* You implemented a gradient descent algorithm\n",
        "* You validated your models using the R2 score\n",
        "* You made dynamic plots in Colab"
      ]
    },
    {
      "metadata": {
        "id": "c07n963s7p_w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ]
    },
    {
      "metadata": {
        "id": "t6HcgdN1hq3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Apply the sklearn LinearRegression() method to a different subset of the Iditarod data by predicting the number of dogs dropped as a function of the elapsed time.\n",
        "\n",
        "You'll need to:\n",
        "* split the data into Train and Test sets\n",
        "* fit a linear model\n",
        "* plot the best fit line over the data\n",
        "* make predictions on y_test given x_test\n",
        "* validate your model using the R2 score on the test data\n",
        "\n",
        "I'll get you started:"
      ]
    },
    {
      "metadata": {
        "id": "5nwgh_0Phe7e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn #import the scikit-learn package\n",
        "from sklearn.model_selection import train_test_split #import the train_test_split function\n",
        "from sklearn.linear_model import LinearRegression # import the LinearRegression module from sklearn\n",
        "from sklearn.metrics import r2_score # import the r2_score module from sklearn\n",
        "\n",
        "x = iditarod[\"Elapsed_time\"]\n",
        "y = iditarod[\"Dogs_dropped\"]\n",
        "\n",
        "plt.scatter(x,y)\n",
        "plt.xlabel(\"Elapsed time (hours)\")\n",
        "plt.ylabel(\"Dogs dropped\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXlfP8FqhnVv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "sklearn has several different linear regressors, including some that are more robust to outliers than is the Least Squares method. In this Exercise, repeat Exercise 1 but use instead the TheilSenRegressor().\n",
        "\n",
        "Some background:\n",
        "* [Wikipedia entry on Theil–Sen estimator](https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator)\n",
        "\n",
        "* [sklearn docs: TheirlSenRegressor](https://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html)\n",
        "\n"
      ]
    }
  ]
}