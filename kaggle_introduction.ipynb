{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_introduction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlites/mlites2019/blob/master/kaggle_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vSp9f2X0ggwa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Kaggle Datasets\n",
        "\n",
        "In this exercise, we'll learn how to get data into Colab\n",
        "\n",
        "First, we'll load up Google Drive\n",
        "\n",
        "Second, we'll search for data stored on Kaggle's servers\n",
        "\n",
        "Finally, we'll download the datasets we'll use in this course"
      ]
    },
    {
      "metadata": {
        "id": "8j_ZYrtGhBX0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive\n",
        "...is easy"
      ]
    },
    {
      "metadata": {
        "id": "0JalKi1VB8mj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "beb7d4a6-dbc1-4cec-b849-c48be19c4f8f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive # from PACKAGE import MODULE\n",
        "drive.mount('/content/gdrive') # mount your Google Drive to the /content/gdrive folder in your VM"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xpAlTBYU9nA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# kill the VM and restart fresh\n",
        "#!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_jaa4bkm8K_8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we'll make a new directory in your Google Drive to work out of. This will make things easier later because the Colab Virtual Machines get recycled every once in a while, so work you've done isn't necessarily persistent across sessions.\n",
        "\n",
        "We'll call the directory 'mlites' and add a link to it in the default /content/ location in Colab"
      ]
    },
    {
      "metadata": {
        "id": "24zB-_ir72xG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir \"/content/gdrive/My Drive/mlites\"\n",
        "!rmdir /content/mlites\n",
        "!ln -s \"/content/gdrive/My Drive/mlites\" \"/content/mlites\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kyyun7qbqZq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up Kaggle\n",
        "\n",
        "...is a bit more complicated\n",
        "\n",
        "[Setting up Kaggle in Colab](https://towardsdatascience.com/setting-up-kaggle-in-google-colab-ebb281b61463)\n",
        "\n",
        "1. Sign up for Kaggle if you're not already a member\n",
        "2. Go to _My Account_\n",
        "3. Go to _Create New API Token_\n",
        "4. That will download a file called **kaggle.json**\n",
        "5. Click *Files* -> *Upload* on the left and upload **kaggle.json**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kcu2pIGhpxQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "1154376c-e987-495e-a5ec-d5f7aa983321"
      },
      "cell_type": "code",
      "source": [
        "# install kaggle package\n",
        "\n",
        "# pip is a python program for installing new packages\n",
        "# the ! indicates this command is to be run on the system's command line\n",
        "!pip install kaggle\n",
        "\n",
        "# set up key authentication\n",
        "# in the Colab VM (Virtual Machine) your user name is 'root'\n",
        "\n",
        "#!mkdir is the system command to make a new directory\n",
        "!mkdir /content/gdrive/kaggle \n",
        "\n",
        "#the . makes it hidden, this is where our credentials will be stored\n",
        "!mkdir /root/.kaggle \n",
        "\n",
        "#copy the file to a place that kaggle expects to find it\n",
        "!mv /content/kaggle.json /root/.kaggle/kaggle.json \n",
        "\n",
        "#change the permissions to avoid leaking your credentials\n",
        "!chmod 600 /root/.kaggle/kaggle.json \n",
        "\n",
        "#setup kaggle to use the /content/kaggle directory we made earlier\n",
        "!kaggle config set -n path -v/content/kaggle \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "- path is now set to: /content/kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UpCVmBMkt8dV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Find Kaggle datasets of interest\n",
        "\n",
        "Kaggle package API details\n",
        "\n",
        "https://github.com/Kaggle/kaggle-api#datasets\n",
        "\n",
        "only the first 20 results are shown, additional pages can be shown with the --page flag"
      ]
    },
    {
      "metadata": {
        "id": "T_7sVX75sYnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1159
        },
        "outputId": "f2111f0d-0480-4690-b2a6-922e2ca68e9d"
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets list --tags oceans #find datasets tagged with 'oceans'\n",
        "!kaggle datasets list --user noaa #find datasets by user 'NOAA'\n",
        "!kaggle datasets list --search environment --page 2 #find page 2 of datasets using search term 'environment'\n",
        "!kaggle datasets list --search alaska"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref                                                           title                                            size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------------  ----------------------------------------------  -----  -------------------  -------------  \n",
            "noaa/noaa-icoads                                              NOAA ICOADS                                     171GB  2018-03-13 17:37:47              0  \n",
            "noaa/deep-sea-corals                                          Deep Sea Corals                                  10MB  2017-08-28 17:11:03            409  \n",
            "uciml/el-nino-dataset                                         El Nino Dataset                                   3MB  2016-11-06 21:02:18           1006  \n",
            "unt/disputed-territories                                      Disputed Territories and Wars, 1816-2001         95KB  2017-02-02 01:46:23            141  \n",
            "teajay/global-shark-attacks                                   Global Shark Attacks                            548KB  2018-07-04 17:59:54           4563  \n",
            "noaa/seismic-waves                                            Tsunami Causes and Waves                        654KB  2017-02-03 04:15:19           1430  \n",
            "antgoldbloom/2016-kitefoil-race-results                       2016 and 2017 Kitefoil Race Results              87KB  2017-10-09 02:34:42            184  \n",
            "oewyn000/humpback-whale-fluke-keypoints                       Humpback Whale Fluke Keypoints                  168MB  2019-01-27 02:32:51             96  \n",
            "polinalemenkova/bathymetry-of-the-mariana-trench-25-profiles  Bathymetry of the Mariana Trench (25 profiles)   20KB  2018-12-09 04:30:50              4  \n",
            "sauuyer/alvin-dives                                           Alvin Dives                                     235KB  2018-05-04 16:11:43             19  \n",
            "ref                                                    title                                              size  lastUpdated          downloadCount  \n",
            "-----------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "noaa/gsod                                              NOAA GSOD                                          26GB  2019-03-20 23:17:46              0  \n",
            "noaa/noaa-icoads                                       NOAA ICOADS                                       171GB  2018-03-13 17:37:47              0  \n",
            "noaa/noaa-precipitation-15-minute                      NOAA Precipitation 15 Minute                      192MB  2019-04-18 15:05:24            283  \n",
            "noaa/noaa-global-surface-summary-of-the-day            NOAA Global Surface Summary of the Day              3GB  2019-04-18 09:09:06            397  \n",
            "noaa/ghcn-d                                            Daily Global Historical Climatology Network        99GB  2019-03-20 23:16:32              0  \n",
            "noaa/noaa-global-historical-climatology-network-daily  NOAA Global Historical Climatology Network Daily   14GB  2019-04-18 18:21:44            296  \n",
            "noaa/goes16                                            NOAA GOES-16                                       24GB  2019-03-20 23:20:37              0  \n",
            "noaa/deep-sea-corals                                   Deep Sea Corals                                    10MB  2017-08-28 17:11:03            409  \n",
            "noaa/severe-weather-data-inventory                     Severe Weather Data Inventory                     182MB  2016-10-24 15:34:45            959  \n",
            "noaa/global-historical-climatology-network             Global Historical Climatology Network               4MB  2016-10-24 15:23:05           1364  \n",
            "noaa/hurricane-database                                Hurricanes and Typhoons, 1851-2014                928KB  2017-01-20 18:15:43           2571  \n",
            "noaa/seismic-waves                                     Tsunami Causes and Waves                          654KB  2017-02-03 04:15:19           1430  \n",
            "noaa/noaa-u-s-climatic-normals                         NOAA U.S. Climatic Normals                         10GB  2019-04-15 14:39:04             48  \n",
            "noaa/noaa-severe-weather-data-inventory                NOAA Severe Weather Data Inventory                 18GB  2019-04-07 12:27:11             97  \n",
            "ref                                                           title                                             size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  \n",
            "buntyshah/food-environment-atlas-data                         Food Environment Atlas Data                        5MB  2018-07-23 10:48:52             40  \n",
            "marshald/london-boroughs                                      London Borough Demographics                       10KB  2017-06-04 13:45:03            346  \n",
            "freedomhouse/press-freedom                                    Freedom of the Press, 2001-2015                   11KB  2017-02-03 23:30:16            307  \n",
            "usdot/nhtsa-traffic-fatalities                                US Traffic Fatality Records                      585MB  2019-03-20 23:21:02              0  \n",
            "ilknuricke/neurohackinginrimages                              Structural MRI Datasets (T1, T2, FLAIR etc.)     190MB  2017-01-04 20:01:18           2698  \n",
            "olistbr/brazilian-ecommerce                                   Brazilian E-Commerce Public Dataset by Olist      42MB  2018-11-29 12:22:57          10703  \n",
            "census/2013-american-community-survey                         2013 American Community Survey                   888MB  2017-05-01 19:10:03           3887  \n",
            "pitasr/scheduling-in-cloud-computing                          Scheduling in Cloud computing                     49KB  2017-06-11 22:07:18            313  \n",
            "regivm/kernel                                                 Manoeuvring Kaggle Kernel and Data Environment     7KB  2018-08-30 08:03:32              2  \n",
            "sohier/calcofi                                                CalCOFI                                           50MB  2017-08-23 19:24:53           2488  \n",
            "sampadab17/network-intrusion-detection                        Network Intrusion Detection                      750KB  2018-10-09 09:39:37            241  \n",
            "usda/a-year-of-pumpkin-prices                                 A Year of Pumpkin Prices                          16KB  2017-10-24 17:45:33           1463  \n",
            "lamdadev/state-wise-tree-cover-india                          State wise tree cover India                       971B  2016-11-10 17:25:47            149  \n",
            "residentmario/database-of-battles                             Historical Military Battles                      129KB  2017-09-13 20:39:07            670  \n",
            "edumagalhaes/quality-prediction-in-a-mining-process           Quality Prediction in a Mining Process            51MB  2017-12-06 21:16:37           1033  \n",
            "fschwartzer/tmd-dataset-5-seconds-sliding-window              TMD Dataset - 5 seconds sliding window             3MB  2019-02-05 18:08:10            145  \n",
            "sohier/mussel-watch                                           Mussel Watch                                      11MB  2017-09-18 16:19:51            891  \n",
            "ihmstefanini/industrial-safety-and-health-analytics-database  Industrial Safety and Health Analytics Database  160KB  2018-04-12 16:51:07            633  \n",
            "sasanj/virtual-reality-driving-simulator-dataset              Virtual Reality Driving Simulator Dataset          9MB  2017-08-16 14:06:11            158  \n",
            "wcukierski/2016-march-ml-mania                                2016 March ML Mania Predictions                   26MB  2017-11-15 22:36:48           1653  \n",
            "ref                                                title                                             size  lastUpdated          downloadCount  \n",
            "-------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  \n",
            "mcdonalds/nutrition-facts                          Nutrition Facts for McDonald's Menu                7KB  2017-03-03 13:30:58          11256  \n",
            "iditarod/iditarod-race                             2017 Iditarod Trail Sled Dog Race                 22KB  2017-03-22 15:04:59            172  \n",
            "noriuk/us-educational-finances                     U.S. Educational Finances                         83MB  2018-08-29 23:47:18           1849  \n",
            "rtatman/188-million-us-wildfires                   1.88 Million US Wildfires                        175MB  2017-09-13 22:41:53           3578  \n",
            "openaddresses/openaddresses-us-west                OpenAddresses - U.S. West                        816MB  2017-08-02 22:49:23            372  \n",
            "jamestollefson/alaskaairfields                     Alaska Airport Data                              266KB  2017-04-20 23:33:32            223  \n",
            "noriuk/us-education-datasets-unification-project   U.S. Education Datasets: Unification Project      85MB  2019-03-02 18:41:52           3534  \n",
            "cms/cms-american-indian-alaska-native-aian-health  CMS American Indian/Alaska Native (AIAN) Health  208KB  2019-04-15 03:30:17              8  \n",
            "jboysen/spy-plane-finder                           Spy Plane Finder                                  25MB  2017-08-11 19:13:10            284  \n",
            "hassenmorad/us-state-baby-names                    US State Baby Names                               41MB  2018-09-14 18:25:40             30  \n",
            "rec3141/biol342-genome-data                        Decontamination of Microbial Genomes              29MB  2019-04-17 16:32:31              3  \n",
            "theriley106/university-statistics                  University Statistics                             33KB  2018-01-21 23:03:55            577  \n",
            "madaha/people-without-internet                     People without internet                           60KB  2018-01-11 16:20:48            280  \n",
            "johnrvg/election1216                               Election 2016                                    630KB  2018-04-11 16:33:34            105  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tn5iOpLEv9H8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iditarod dataset\n",
        "\n",
        "Now find the specific files you want and download them\n",
        "\n",
        "Let's see what files are available for the 2017 Iditarod dataset"
      ]
    },
    {
      "metadata": {
        "id": "vCWpgYW8wACG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8c1b2d35-4900-4edf-9628-8c295d7aa15c"
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets files iditarod/iditarod-race"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name         size  creationDate         \n",
            "----------  -----  -------------------  \n",
            "report.csv  139KB  2017-03-22 15:03:30  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8oFjm_yzwonh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's download all the files, they'll show up in /content/kaggle/datasets"
      ]
    },
    {
      "metadata": {
        "id": "ktpZKadxwqg9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "34e287c0-0405-4d4e-d75d-3a083b8ec911"
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download iditarod/iditarod-race"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading iditarod-race.zip to /content/kaggle/datasets/iditarod/iditarod-race\n",
            "\r  0% 0.00/21.5k [00:00<?, ?B/s]\n",
            "\r100% 21.5k/21.5k [00:00<00:00, 17.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZMIlCZctyq7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "69343719-2a48-4a7e-d8b6-1547a63b6aeb"
      },
      "cell_type": "code",
      "source": [
        "#unzip the files\n",
        "!unzip /content/kaggle/datasets/iditarod/iditarod-race/iditarod-race.zip -d ./"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/kaggle/datasets/iditarod/iditarod-race/iditarod-race.zip\n",
            "  inflating: ./report.csv            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "syUDb3mdzmPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "outputId": "d5146f70-da56-43a2-f700-9074d11b4d45"
      },
      "cell_type": "code",
      "source": [
        "# let's rename it to something more useful\n",
        "!mv report.csv iditarod.csv\n",
        "\n",
        "# and take a quick look\n",
        "import pandas as pd # pandas is library providing high-performance, easy-to-use data structures and data analysis tools\n",
        "iditarod = pd.read_csv('iditarod.csv') # reads the CSV file into a Panda dataframe\n",
        "iditarod.shape # the shape is the number of rows, columns in the dataframe\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'report.csv': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ebb7280bf783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# and take a quick look\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# pandas is library providing high-performance, easy-to-use data structures and data analysis tools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0miditarod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iditarod.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reads the CSV file into a Panda dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0miditarod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m# the shape is the number of rows, columns in the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'iditarod.csv' does not exist"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "J3dNlrGI2BdC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iditarod.head(10) # head() is a command to show the top few lines of the file, in this case 10\n",
        "iditarod.tail(10) # tail() shows the bottom\n",
        "\n",
        "#note that the jupyter notebook only displays the last command printed here, as a nicely formatted table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jxqykLVjSqEi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BIOL342 Dataset\n",
        "\n",
        "Now we'll download the data that we'll use for examples in this course"
      ]
    },
    {
      "metadata": {
        "id": "X01qpm1kSpkB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets list --user rec3141\n",
        "!kaggle datasets files rec3141/biol342-genome-data\n",
        "!kaggle datasets download rec3141/biol342-genome-data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4OBXQYvTLWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/datasets/rec3141/biol342-genome-data/biol342-genome-data.zip -d ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKJMxWcrnaAa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "covlengc = pd.read_csv('biol342_cov_len_gc.tsv',sep='\\t') #read in a TAB separated file\n",
        "covlengc.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OWLU-vXdn4QN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "covlengc.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWJPplOjTd2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "covlengc.tail(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ihos-YLzoBVO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The End\n",
        "\n",
        "Nice job!\n",
        "\n",
        "In this lesson we learned:\n",
        "\n",
        "1. How to mount our Google Drive into our Jupyter notebook\n",
        "2. How to setup the Kaggle package in our Jupyter notebook\n",
        "3. How to search for and download Kaggle datasets using **!kaggle datasets**\n",
        "4. How to perform system commands like cp, mv, mkdir, unzip, chmod, and pip on the command line using **!**\n",
        "5. How to import a text file into a pandas dataframe\n",
        "6. How to preview and find some basic information about the data using **pd.head()**, **pd.tail()**, and the **shape** attribute"
      ]
    }
  ]
}